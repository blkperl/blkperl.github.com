<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Evacuating a Ganeti node for hardware diagnostics</title>
        <link rel="stylesheet" href="http://blkperl.github.com/theme/css/main.css" />
        <link href="http://blkperl.github.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="puppet resource blog author=blkperl Atom Feed" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://blkperl.github.com/">puppet resource blog author=blkperl </a></h1>
                <nav><ul>
                    <li><a href="http://blkperl.github.com/category/centos.html">centos</a></li>
                    <li class="active"><a href="http://blkperl.github.com/category/ganeti.html">ganeti</a></li>
                    <li><a href="http://blkperl.github.com/category/lxc.html">lxc</a></li>
                    <li><a href="http://blkperl.github.com/category/puppet.html">puppet</a></li>
                    <li><a href="http://blkperl.github.com/category/solaris.html">solaris</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="http://blkperl.github.com/evac-ganeti-node.html" rel="bookmark"
           title="Permalink to Evacuating a Ganeti node for hardware diagnostics">Evacuating a Ganeti node for hardware diagnostics</a></h1>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="pdx_blkperl">Tweet</a><script type="text/javascript" src="//platform.twitter.com/widgets.js"></script>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2013-03-23T12:00:00-07:00">
                Published: Sat 23 March 2013
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="http://blkperl.github.com/author/william-van-hevelingen.html">William Van Hevelingen</a>
        </address>
<p>In <a href="http://blkperl.github.com/category/ganeti.html">ganeti</a>. </p>
<p>tags: <a href="http://blkperl.github.com/tag/ganeti.html">ganeti</a> <a href="http://blkperl.github.com/tag/kvm.html">kvm</a> </p>
</footer><!-- /.post-info -->      <p>One of the nodes in our Ganeti cluster was hanging on lvs commands and some of the instances were hanging while they wait for IO. I decided to be proactive and live migrate all of the instances off the node in order to bring it down for some debugging.</p>
<p>The following steps are how I brought the node offline with minimal downtime for production instances.</p>
<h2>Overview of our cluster</h2>
<ul>
<li>4 nodes</li>
<li>46 instances</li>
<li>Ubuntu 12.04 LTS</li>
<li>Ganeti version 2.5.2-1</li>
<li>Kvm version 1.0</li>
</ul>
<h2>Log into the master node</h2>
<p>Log into the master ganeti node.</p>
<div class="highlight"><pre>ssh nebula.cat.pdx.edu
</pre></div>


<h2>Migrate the primary instances off suspect node</h2>
<p>Run the gnt-node migrate command, passing in the node to migrate off of</p>
<div class="highlight"><pre>sudo gnt-node migrate katana.cat.pdx.edu
</pre></div>


<p>The output should look similar to this.</p>
<div class="highlight"><pre>root@claymore:~# gnt-node migrate katana
Migrate instance<span class="o">(</span>s<span class="o">)</span> crystal.cat.pdx.edu, l1011.cat.pdx.edu,
marauder.cat.pdx.edu, nyan.cat.pdx.edu, panic.cat.pdx.edu,
receptacle.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu,
webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu?
y/<span class="o">[</span>n<span class="o">]</span>/?: y
Submitted <span class="nb">jobs </span>52820, 52821, 52822, 52823, 52824, 52825, 52826, 52827, 52828, 52829, 52830
<span class="o">[</span>...<span class="o">]</span>
Waiting <span class="k">for</span> job <span class="m">52825</span> ...
Wed Mar <span class="m">20</span> 10:33:52 <span class="m">2013</span> Migrating instance crystal.cat.pdx.edu
Wed Mar <span class="m">20</span> 10:33:52 <span class="m">2013</span> * checking disk consistency between <span class="nb">source </span>and target
Wed Mar <span class="m">20</span> 10:33:56 <span class="m">2013</span> * switching node claymore.cat.pdx.edu to secondary mode
Wed Mar <span class="m">20</span> 10:33:56 <span class="m">2013</span> * changing into standalone mode
Wed Mar <span class="m">20</span> 10:34:58 <span class="m">2013</span> * changing disks into dual-master mode
Wed Mar <span class="m">20</span> 10:35:01 <span class="m">2013</span> * <span class="nb">wait </span><span class="k">until</span> resync is <span class="k">done</span>
Wed Mar <span class="m">20</span> 10:35:02 <span class="m">2013</span> * preparing claymore.cat.pdx.edu to accept the instance
Wed Mar <span class="m">20</span> 10:35:03 <span class="m">2013</span> * migrating instance to claymore.cat.pdx.edu
Wed Mar <span class="m">20</span> 10:35:24 <span class="m">2013</span> * switching node katana.cat.pdx.edu to secondary mode
Wed Mar <span class="m">20</span> 10:35:26 <span class="m">2013</span> * <span class="nb">wait </span><span class="k">until</span> resync is <span class="k">done</span>
Wed Mar <span class="m">20</span> 10:35:27 <span class="m">2013</span> * changing into standalone mode
Wed Mar <span class="m">20</span> 10:35:27 <span class="m">2013</span> * changing disks into single-master mode
Wed Mar <span class="m">20</span> 10:35:28 <span class="m">2013</span> * <span class="nb">wait </span><span class="k">until</span> resync is <span class="k">done</span>
Wed Mar <span class="m">20</span> 10:35:28 <span class="m">2013</span> * <span class="k">done</span>
<span class="o">[</span>...<span class="o">]</span>
</pre></div>


<p>Some of my instances failed to migrate properly. This seems to be related to this <a href="https://code.google.com/p/ganeti/issues/detail?id=297">bug</a>.</p>
<div class="highlight"><pre>Wed Mar <span class="m">20</span> 10:33:16 <span class="m">2013</span> Migration failed, aborting
<span class="o">[</span>...<span class="o">]</span>
Job <span class="m">52820</span> has failed: Failure: <span class="nb">command </span>execution error:
Could not migrate instance sapphire.cat.pdx.edu: Failed to migrate instance: Too many <span class="s1">&#39;info migrate&#39;</span> broken answers
</pre></div>


<p>Luckly these instances were not being used at the moment and I could use gnt-instance failover.</p>
<div class="highlight"><pre><span class="c"># Warning! This will reboot the instance</span>
sudo gnt-instance failover sapphire.cat.pdx.edu
</pre></div>


<h2>Evacuate secondaries using gnt-node</h2>
<p>After the primaries have been migrated run the gnt-node evacuate command to move the secondaries.</p>
<div class="highlight"><pre>sudo gnt-node evacuate --secondary-only node2.cat.pdx.edu
</pre></div>


<p>gnt-node evacuate failed with a timeout error.</p>
<div class="highlight"><pre>sudo gnt-node evacuate --secondary-only katana
Relocate instance<span class="o">(</span>s<span class="o">)</span> crystal.cat.pdx.edu, emerald.cat.pdx.edu,
<span class="o">[</span>...<span class="o">]</span>
y/<span class="o">[</span>n<span class="o">]</span>/?: y
Wed Mar <span class="m">20</span> 11:16:49 <span class="m">2013</span>  - INFO: Evacuating instances from node <span class="s1">&#39;katana.cat.pdx.edu&#39;</span>: crystal.cat.pdx.edu, emerald.cat.pdx.edu, fog.cat.pdx.edu, gameandwatch.cat.pdx.edu, l1011.cat.pdx.edu, log.cat.pdx.edu, marauder.cat.pdx.edu, nydus.cat.pdx.edu, panic.cat.pdx.edu, pika.cat.pdx.edu, receptacle.cat.pdx.edu, refinery.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu, void.cat.pdx.edu, warpgate.cat.pdx.edu, weba.cat.pdx.edu, webb.cat.pdx.edu, webc.cat.pdx.edu, webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu
Failure: <span class="nb">command </span>execution error:
Can<span class="err">&#39;</span>t get data <span class="k">for</span> node katana.cat.pdx.edu: Error 28: Operation timed out after <span class="m">60910</span> milliseconds with <span class="m">0</span> out of -1 bytes received
</pre></div>


<p>If gnt-node evacuate fails for you can evacuate secondaries per instance.</p>
<h2>Evacuate the secondaries using gnt-instance</h2>
<p>Locate the instances you want to move.</p>
<div class="highlight"><pre>sudo gnt-instance list -o name,pnode,snodes <span class="p">|</span> grep katana

gameandwatch.cat.pdx.edu claymore.cat.pdx.edu katana.cat.pdx.edu
l1011.cat.pdx.edu        rapier.cat.pdx.edu   katana.cat.pdx.edu
log.cat.pdx.edu          dirk.cat.pdx.edu     katana.cat.pdx.edu
marauder.cat.pdx.edu     rapier.cat.pdx.edu   katana.cat.pdx.edu
yermom.cat.pdx.edu       dirk.cat.pdx.edu     katana.cat.pdx.edu
</pre></div>


<p>Run gnt-instance replace-disks and give the name of another node that is not the primary of the instance.</p>
<div class="highlight"><pre>sudo gnt-instance replace-disks -n claymore.cat.pdx.edu yermom.cat.pdx.edu
</pre></div>


<p>The output will look similar to this:</p>
<div class="highlight"><pre>Wed Mar <span class="m">20</span> 13:24:39 <span class="m">2013</span> Replacing disk<span class="o">(</span>s<span class="o">)</span> <span class="m">0</span> <span class="k">for</span> yermom.cat.pdx.edu
Wed Mar <span class="m">20</span> 13:24:39 <span class="m">2013</span> STEP 1/6 Check device existence
Wed Mar <span class="m">20</span> 13:24:39 <span class="m">2013</span>  - INFO: Checking disk/0 on rapier.cat.pdx.edu
Wed Mar <span class="m">20</span> 13:24:40 <span class="m">2013</span>  - INFO: Checking volume groups
Wed Mar <span class="m">20</span> 13:24:40 <span class="m">2013</span> STEP 2/6 Check peer consistency
Wed Mar <span class="m">20</span> 13:24:40 <span class="m">2013</span>  - INFO: Checking disk/0 consistency on node rapier.cat.pdx.edu
Wed Mar <span class="m">20</span> 13:24:40 <span class="m">2013</span> STEP 3/6 Allocate new storage
Wed Mar <span class="m">20</span> 13:24:40 <span class="m">2013</span>  - INFO: Adding new <span class="nb">local </span>storage on claymore.cat.pdx.edu <span class="k">for</span> disk/0
Wed Mar <span class="m">20</span> 13:24:44 <span class="m">2013</span> STEP 4/6 Changing drbd configuration
Wed Mar <span class="m">20</span> 13:24:44 <span class="m">2013</span>  - INFO: activating a new drbd on claymore.cat.pdx.edu <span class="k">for</span> disk/0
Wed Mar <span class="m">20</span> 13:24:48 <span class="m">2013</span>  - INFO: Shutting down drbd <span class="k">for</span> disk/0 on old node
Wed Mar <span class="m">20</span> 13:24:48 <span class="m">2013</span>  - INFO: Detaching primary drbds from the network <span class="o">(=</span>&gt; standalone<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:24:49 <span class="m">2013</span>  - INFO: Updating instance configuration
Wed Mar <span class="m">20</span> 13:24:49 <span class="m">2013</span>  - INFO: Attaching primary drbds to new secondary <span class="o">(</span><span class="nv">standalone</span> <span class="o">=</span>&gt; connected<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:24:50 <span class="m">2013</span> STEP 5/6 Sync devices
Wed Mar <span class="m">20</span> 13:24:50 <span class="m">2013</span>  - INFO: Waiting <span class="k">for</span> instance yermom.cat.pdx.edu to sync disks.
Wed Mar <span class="m">20</span> 13:24:55 <span class="m">2013</span>  - INFO: - device disk/0:  0.50% <span class="k">done</span>, 13m 34s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:26:00 <span class="m">2013</span>  - INFO: - device disk/0:  6.90% <span class="k">done</span>, 15m 53s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:27:07 <span class="m">2013</span>  - INFO: - device disk/0: 13.40% <span class="k">done</span>, 14m 42s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:28:26 <span class="m">2013</span>  - INFO: - device disk/0: 21.00% <span class="k">done</span>, 13m 1s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:29:30 <span class="m">2013</span>  - INFO: - device disk/0: 27.00% <span class="k">done</span>, 14m 32s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:30:35 <span class="m">2013</span>  - INFO: - device disk/0: 33.20% <span class="k">done</span>, 11m 27s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:31:40 <span class="m">2013</span>  - INFO: - device disk/0: 39.50% <span class="k">done</span>, 9m 56s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:32:44 <span class="m">2013</span>  - INFO: - device disk/0: 45.80% <span class="k">done</span>, 9m 20s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:32:44 <span class="m">2013</span>  - INFO: - device disk/0: 45.80% <span class="k">done</span>, 9m 20s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:33:49 <span class="m">2013</span>  - INFO: - device disk/0: 52.10% <span class="k">done</span>, 8m 2s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:34:54 <span class="m">2013</span>  - INFO: - device disk/0: 58.30% <span class="k">done</span>, 6m 54s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:35:58 <span class="m">2013</span>  - INFO: - device disk/0: 64.60% <span class="k">done</span>, 5m 48s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:37:03 <span class="m">2013</span>  - INFO: - device disk/0: 70.90% <span class="k">done</span>, 4m 52s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:38:08 <span class="m">2013</span>  - INFO: - device disk/0: 77.20% <span class="k">done</span>, 3m 50s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:39:15 <span class="m">2013</span>  - INFO: - device disk/0: 83.70% <span class="k">done</span>, 2m 33s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:40:20 <span class="m">2013</span>  - INFO: - device disk/0: 90.00% <span class="k">done</span>, 1m 44s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:41:25 <span class="m">2013</span>  - INFO: - device disk/0: 96.30% <span class="k">done</span>, 37s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar <span class="m">20</span> 13:42:04 <span class="m">2013</span>  - INFO: Instance yermom.cat.pdx.edu<span class="err">&#39;</span>s disks are in sync.
Wed Mar <span class="m">20</span> 13:42:04 <span class="m">2013</span> STEP 6/6 Removing old storage
Wed Mar <span class="m">20</span> 13:42:04 <span class="m">2013</span>  - INFO: Remove logical volumes <span class="k">for</span> 0
</pre></div>


<p>Sometimes you may get an lvm error that looks like this:</p>
<div class="highlight"><pre>Wed Mar <span class="m">20</span> 13:15:18 <span class="m">2013</span>  - WARNING: Can<span class="s1">&#39;t remove old LV: Can&#39;</span>t lvremove: exited with <span class="nb">exit </span>code <span class="m">5</span> -   Unable to deactivate open ganeti-9bc9b089--59f3-
-4512--9a09--e7f756caadbe.disk0_meta <span class="o">(</span>252:7<span class="o">)</span><span class="se">\n</span>  Unable to deactivate logical volume <span class="s2">&quot;9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta&quot;</span><span class="se">\n</span>
Wed Mar <span class="m">20</span> 13:15:18 <span class="m">2013</span>       Hint: remove unused LVs manually
</pre></div>


<p>If this happens you can manually remove the old volume with the lvremove command</p>
<div class="highlight"><pre><span class="c"># Log into the node</span>
ssh katana.cat.pdx.edu
lvremove /dev/mapper/ganeti-9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta
</pre></div>


<h2>Mark the node offline</h2>
<p>The last step is to mark the node offline</p>
<div class="highlight"><pre>sudo gnt-node modify -O yes katana.cat.pdx.edu
</pre></div>


<p>gnt-cluster verify should now display that one node is offline</p>
<div class="highlight"><pre>gnt-cluster verify 
<span class="o">[</span>..<span class="o">]</span>
Sat Mar <span class="m">23</span> 14:18:14 <span class="m">2013</span> * Other Notes
Sat Mar <span class="m">23</span> 14:18:14 <span class="m">2013</span>   - NOTICE: <span class="m">1</span> offline node<span class="o">(</span>s<span class="o">)</span> found.
<span class="o">[</span>..<span class="o">]</span>
</pre></div>


<p>And that is it, you can now take the node offline for diagnostics or to wait for new parts.</p>
<h2>Rebalance the cluster</h2>
<p>At this point you probably want to rebalance your Ganeti cluser. The following blog post by Lance Albertson does a pretty good job at explaining this process.</p>
<p><a href="http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/">http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/</a></p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="http://blkperl.github.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://twitter.com/pdx_blkperl">twitter</a></li>
                            <li><a href="https://github.com/blkperl">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>