<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Evacuating a Ganeti node for hardware diagnostics</title>
        <link rel="stylesheet" href="http://blkperl.github.com/theme/css/main.css" />
        <link href="http://blkperl.github.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="zfs snapshot blkperl/dev/brain@Today Atom Feed" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://blkperl.github.com/">zfs snapshot blkperl/dev/brain@Today </a></h1>
                <nav><ul>
                    <li><a href="http://blkperl.github.com/category/centos.html">centos</a></li>
                    <li class="active"><a href="http://blkperl.github.com/category/ganeti.html">ganeti</a></li>
                    <li><a href="http://blkperl.github.com/category/lxc.html">lxc</a></li>
                    <li><a href="http://blkperl.github.com/category/puppet.html">puppet</a></li>
                    <li><a href="http://blkperl.github.com/category/solaris.html">solaris</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="http://blkperl.github.com/evac-ganeti-node.html" rel="bookmark"
           title="Permalink to Evacuating a Ganeti node for hardware diagnostics">Evacuating a Ganeti node for hardware diagnostics</a></h1>
<a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal" data-via="pdx_blkperl">Tweet</a><script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2013-03-23T12:00:00">
                Sat 23 March 2013
        </abbr>

        <address class="vcard author">
                By <a class="url fn" href="http://blkperl.github.com/author/william-van-hevelingen.html">William Van Hevelingen</a>
        </address>
<p>In <a href="http://blkperl.github.com/category/ganeti.html">ganeti</a>. </p>
<p>tags: <a href="http://blkperl.github.com/tag/ganeti.html">ganeti</a><a href="http://blkperl.github.com/tag/kvm.html">kvm</a></p>
</footer><!-- /.post-info -->      <p>One of the nodes in our Ganeti cluster was hanging on lvs commands and some of the instances were hanging while they wait for IO. I decided to be proactive and live migrate all of the instances off the node in order to bring it down for some debugging.</p>
<p>The following steps are how I brought the node offline with minimal downtime for production instances.</p>
<h2>Overview of our cluster</h2>
<ul>
<li>4 nodes</li>
<li>46 instances</li>
<li>Ubuntu 12.04 LTS</li>
<li>Ganeti version 2.5.2-1</li>
<li>Kvm version 1.0</li>
</ul>
<h2>Log into the master node</h2>
<p>Log into the master ganeti node.</p>
<div class="highlight"><pre>ssh nebula.cat.pdx.edu
</pre></div>


<h2>Migrate the primary instances off suspect node</h2>
<p>Run the gnt-node migrate command, passing in the node to migrate off of</p>
<div class="highlight"><pre><span class="n">sudo</span> <span class="n">gnt</span><span class="o">-</span><span class="n">node</span> <span class="n">migrate</span> <span class="n">katana</span><span class="p">.</span><span class="n">cat</span><span class="p">.</span><span class="n">pdx</span><span class="p">.</span><span class="n">edu</span>
</pre></div>


<p>The output should look similar to this.</p>
<div class="highlight"><pre>root@claymore:~# gnt-node migrate katana
Migrate instance<span class="o">(</span>s<span class="o">)</span> crystal.cat.pdx.edu, l1011.cat.pdx.edu,
marauder.cat.pdx.edu, nyan.cat.pdx.edu, panic.cat.pdx.edu,
receptacle.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu,
webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu?
y/<span class="o">[</span>n<span class="o">]</span>/?: y
Submitted <span class="nb">jobs </span>52820, 52821, 52822, 52823, 52824, 52825, 52826, 52827, 52828, 52829, 52830
<span class="o">[</span>...<span class="o">]</span>
Waiting <span class="k">for </span>job 52825 ...
Wed Mar 20 10:33:52 2013 Migrating instance crystal.cat.pdx.edu
Wed Mar 20 10:33:52 2013 * checking disk consistency between <span class="nb">source </span>and target
Wed Mar 20 10:33:56 2013 * switching node claymore.cat.pdx.edu to secondary mode
Wed Mar 20 10:33:56 2013 * changing into standalone mode
Wed Mar 20 10:34:58 2013 * changing disks into dual-master mode
Wed Mar 20 10:35:01 2013 * <span class="nb">wait </span><span class="k">until </span>resync is <span class="k">done</span>
Wed Mar 20 10:35:02 2013 * preparing claymore.cat.pdx.edu to accept the instance
Wed Mar 20 10:35:03 2013 * migrating instance to claymore.cat.pdx.edu
Wed Mar 20 10:35:24 2013 * switching node katana.cat.pdx.edu to secondary mode
Wed Mar 20 10:35:26 2013 * <span class="nb">wait </span><span class="k">until </span>resync is <span class="k">done</span>
Wed Mar 20 10:35:27 2013 * changing into standalone mode
Wed Mar 20 10:35:27 2013 * changing disks into single-master mode
Wed Mar 20 10:35:28 2013 * <span class="nb">wait </span><span class="k">until </span>resync is <span class="k">done</span>
Wed Mar 20 10:35:28 2013 * <span class="k">done</span>
<span class="o">[</span>...<span class="o">]</span>
</pre></div>


<p>Some of my instances failed to migrate properly. This seems to be related to this <a href="https://code.google.com/p/ganeti/issues/detail?id=297">bug</a>.</p>
<div class="highlight"><pre>Wed Mar 20 10:33:16 2013 Migration failed, aborting
<span class="o">[</span>...<span class="o">]</span>
Job 52820 has failed: Failure: <span class="nb">command </span>execution error:
Could not migrate instance sapphire.cat.pdx.edu: Failed to migrate instance: Too many <span class="s1">&#39;info migrate&#39;</span> broken answers
</pre></div>


<p>Luckly these instances were not being used at the moment and I could use gnt-instance failover.</p>
<div class="highlight"><pre><span class="c"># Warning! This will reboot the instance</span>
sudo gnt-instance failover sapphire.cat.pdx.edu
</pre></div>


<h2>Evacuate secondaries using gnt-node</h2>
<p>After the primaries have been migrated run the gnt-node evacuate command to move the secondaries.</p>
<div class="highlight"><pre>sudo gnt-node evacuate --secondary-only node2.cat.pdx.edu
</pre></div>


<p>gnt-node evacuate failed with a timeout error.</p>
<div class="highlight"><pre>sudo gnt-node evacuate --secondary-only katana
Relocate instance<span class="o">(</span>s<span class="o">)</span> crystal.cat.pdx.edu, emerald.cat.pdx.edu,
<span class="o">[</span>...<span class="o">]</span>
y/<span class="o">[</span>n<span class="o">]</span>/?: y
Wed Mar 20 11:16:49 2013  - INFO: Evacuating instances from node <span class="s1">&#39;katana.cat.pdx.edu&#39;</span>: crystal.cat.pdx.edu, emerald.cat.pdx.edu, fog.cat.pdx.edu, gameandwatch.cat.pdx.edu, l1011.cat.pdx.edu, log.cat.pdx.edu, marauder.cat.pdx.edu, nydus.cat.pdx.edu, panic.cat.pdx.edu, pika.cat.pdx.edu, receptacle.cat.pdx.edu, refinery.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu, void.cat.pdx.edu, warpgate.cat.pdx.edu, weba.cat.pdx.edu, webb.cat.pdx.edu, webc.cat.pdx.edu, webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu
Failure: <span class="nb">command </span>execution error:
Can<span class="err">&#39;</span>t get data <span class="k">for </span>node katana.cat.pdx.edu: Error 28: Operation timed out after 60910 milliseconds with 0 out of -1 bytes received
</pre></div>


<p>If gnt-node evacuate fails for you can evacuate secondaries per instance.</p>
<h2>Evacuate the secondaries using gnt-instance</h2>
<p>Locate the instances you want to move.</p>
<div class="highlight"><pre>sudo gnt-instance list -o name,pnode,snodes | grep katana

gameandwatch.cat.pdx.edu claymore.cat.pdx.edu katana.cat.pdx.edu
l1011.cat.pdx.edu        rapier.cat.pdx.edu   katana.cat.pdx.edu
log.cat.pdx.edu          dirk.cat.pdx.edu     katana.cat.pdx.edu
marauder.cat.pdx.edu     rapier.cat.pdx.edu   katana.cat.pdx.edu
yermom.cat.pdx.edu       dirk.cat.pdx.edu     katana.cat.pdx.edu
</pre></div>


<p>Run gnt-instance replace-disks and give the name of another node that is not the primary of the instance.</p>
<div class="highlight"><pre>sudo gnt-instance replace-disks -n claymore.cat.pdx.edu yermom.cat.pdx.edu
</pre></div>


<p>The output will look similar to this:</p>
<div class="highlight"><pre>Wed Mar 20 13:24:39 2013 Replacing disk<span class="o">(</span>s<span class="o">)</span> 0 <span class="k">for </span>yermom.cat.pdx.edu
Wed Mar 20 13:24:39 2013 STEP 1/6 Check device existence
Wed Mar 20 13:24:39 2013  - INFO: Checking disk/0 on rapier.cat.pdx.edu
Wed Mar 20 13:24:40 2013  - INFO: Checking volume groups
Wed Mar 20 13:24:40 2013 STEP 2/6 Check peer consistency
Wed Mar 20 13:24:40 2013  - INFO: Checking disk/0 consistency on node rapier.cat.pdx.edu
Wed Mar 20 13:24:40 2013 STEP 3/6 Allocate new storage
Wed Mar 20 13:24:40 2013  - INFO: Adding new <span class="nb">local </span>storage on claymore.cat.pdx.edu <span class="k">for </span>disk/0
Wed Mar 20 13:24:44 2013 STEP 4/6 Changing drbd configuration
Wed Mar 20 13:24:44 2013  - INFO: activating a new drbd on claymore.cat.pdx.edu <span class="k">for </span>disk/0
Wed Mar 20 13:24:48 2013  - INFO: Shutting down drbd <span class="k">for </span>disk/0 on old node
Wed Mar 20 13:24:48 2013  - INFO: Detaching primary drbds from the network <span class="o">(=</span>&gt; standalone<span class="o">)</span>
Wed Mar 20 13:24:49 2013  - INFO: Updating instance configuration
Wed Mar 20 13:24:49 2013  - INFO: Attaching primary drbds to new secondary <span class="o">(</span><span class="nv">standalone</span> <span class="o">=</span>&gt; connected<span class="o">)</span>
Wed Mar 20 13:24:50 2013 STEP 5/6 Sync devices
Wed Mar 20 13:24:50 2013  - INFO: Waiting <span class="k">for </span>instance yermom.cat.pdx.edu to sync disks.
Wed Mar 20 13:24:55 2013  - INFO: - device disk/0:  0.50% <span class="k">done</span>, 13m 34s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:26:00 2013  - INFO: - device disk/0:  6.90% <span class="k">done</span>, 15m 53s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:27:07 2013  - INFO: - device disk/0: 13.40% <span class="k">done</span>, 14m 42s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:28:26 2013  - INFO: - device disk/0: 21.00% <span class="k">done</span>, 13m 1s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:29:30 2013  - INFO: - device disk/0: 27.00% <span class="k">done</span>, 14m 32s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:30:35 2013  - INFO: - device disk/0: 33.20% <span class="k">done</span>, 11m 27s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:31:40 2013  - INFO: - device disk/0: 39.50% <span class="k">done</span>, 9m 56s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:32:44 2013  - INFO: - device disk/0: 45.80% <span class="k">done</span>, 9m 20s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:32:44 2013  - INFO: - device disk/0: 45.80% <span class="k">done</span>, 9m 20s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:33:49 2013  - INFO: - device disk/0: 52.10% <span class="k">done</span>, 8m 2s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:34:54 2013  - INFO: - device disk/0: 58.30% <span class="k">done</span>, 6m 54s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:35:58 2013  - INFO: - device disk/0: 64.60% <span class="k">done</span>, 5m 48s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:37:03 2013  - INFO: - device disk/0: 70.90% <span class="k">done</span>, 4m 52s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:38:08 2013  - INFO: - device disk/0: 77.20% <span class="k">done</span>, 3m 50s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:39:15 2013  - INFO: - device disk/0: 83.70% <span class="k">done</span>, 2m 33s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:40:20 2013  - INFO: - device disk/0: 90.00% <span class="k">done</span>, 1m 44s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:41:25 2013  - INFO: - device disk/0: 96.30% <span class="k">done</span>, 37s remaining <span class="o">(</span>estimated<span class="o">)</span>
Wed Mar 20 13:42:04 2013  - INFO: Instance yermom.cat.pdx.edu<span class="err">&#39;</span>s disks are in sync.
Wed Mar 20 13:42:04 2013 STEP 6/6 Removing old storage
Wed Mar 20 13:42:04 2013  - INFO: Remove logical volumes <span class="k">for </span>0
</pre></div>


<p>Sometimes you may get an lvm error that looks like this:</p>
<div class="highlight"><pre>Wed Mar 20 13:15:18 2013  - WARNING: Can<span class="s1">&#39;t remove old LV: Can&#39;</span>t lvremove: exited with <span class="nb">exit </span>code 5 -   Unable to deactivate open ganeti-9bc9b089--59f3-
-4512--9a09--e7f756caadbe.disk0_meta <span class="o">(</span>252:7<span class="o">)</span><span class="se">\n</span>  Unable to deactivate logical volume <span class="s2">&quot;9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta&quot;</span><span class="se">\n</span>
Wed Mar 20 13:15:18 2013       Hint: remove unused LVs manually
</pre></div>


<p>If this happens you can manually remove the old volume with the lvremove command</p>
<div class="highlight"><pre><span class="c"># Log into the node</span>
ssh katana.cat.pdx.edu
lvremove /dev/mapper/ganeti-9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta
</pre></div>


<h2>Mark the node offline</h2>
<p>The last step is to mark the node offline</p>
<div class="highlight"><pre>sudo gnt-node modify -O yes katana.cat.pdx.edu
</pre></div>


<p>gnt-cluster verify should now display that one node is offline</p>
<div class="highlight"><pre>gnt-cluster verify 
<span class="o">[</span>..<span class="o">]</span>
Sat Mar 23 14:18:14 2013 * Other Notes
Sat Mar 23 14:18:14 2013   - NOTICE: 1 offline node<span class="o">(</span>s<span class="o">)</span> found.
<span class="o">[</span>..<span class="o">]</span>
</pre></div>


<p>And that is it, you can now take the node offline for diagnostics or to wait for new parts.</p>
<h2>Rebalance the cluster</h2>
<p>At this point you probably want to rebalance your Ganeti cluser. The following blog post by Lance Albertson does a pretty good job at explaining this process.</p>
<p><a href="http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/">http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/</a></p>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="http://blkperl.github.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="http://twitter.com/pdx_blkperl">twitter</a></li>
                            <li><a href="http://github.com/blkperl">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>