<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>puppet resource blog author=blkperl</title><link href="http://blkperl.github.com/" rel="alternate"></link><link href="http://blkperl.github.com/feeds/ganeti.atom.xml" rel="self"></link><id>http://blkperl.github.com/</id><updated>2013-05-24T23:08:00-07:00</updated><entry><title>Fixing a split brain Ganeti instance</title><link href="http://blkperl.github.com/split-brain-ganeti.html" rel="alternate"></link><updated>2013-05-24T23:08:00-07:00</updated><author><name>William Van Hevelingen</name></author><id>tag:blkperl.github.com,2013-05-24:split-brain-ganeti.html</id><summary type="html">&lt;h2&gt;Recovering from split brain&lt;/h2&gt;
&lt;p&gt;We run a 4 node Ganeti cluster and during a failover of a node some instances got degraded disks. We're not sure how it happened but some quick googling told us it was a split brain and is recoverable. The following is how we confirmed it was split brain and how we repaired the affected instances.&lt;/p&gt;
&lt;h2&gt;You can identify a split brain by the following.&lt;/h2&gt;
&lt;p&gt;Degraded disks in gnt-instance info&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;on primary:   /dev/drbd1 &lt;span class="o"&gt;(&lt;/span&gt;147:1&lt;span class="o"&gt;)&lt;/span&gt; in sync, status *DEGRADED*
on secondary: /dev/drbd9 &lt;span class="o"&gt;(&lt;/span&gt;147:9&lt;span class="o"&gt;)&lt;/span&gt; in sync, status *DEGRADED*
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;StandAlone state on the primary (/proc/drbd)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt; 1: cs:StandAlone ro:Primary/Unknown ds:UpToDate/DUnknown   r-----
    ns:969536 nr:0 dw:22564060 dr:43036016 al:242 bm:2652 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:254024
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;StandAlone state on the secondary (/proc/drbd)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;9: cs:StandAlone ro:Primary/Unknown ds:UpToDate/DUnknown   r-----
    ns:0 nr:969536 dw:24185104 dr:996 al:0 bm:1293 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Steps to repair&lt;/h1&gt;
&lt;h2&gt;Recreate the secondary&lt;/h2&gt;
&lt;p&gt;(assuming you think the primary is healthy)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# replace $another_node with any node that is not the primary or secondary&lt;/span&gt;
gnt-instance replace-disks -n &lt;span class="nv"&gt;$another_node&lt;/span&gt; &lt;span class="nv"&gt;$instance&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Wait for disks to re-sync&lt;/h2&gt;
&lt;p&gt;You can watch the progress by looking at /proc/drbd&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;1: cs:SyncSource ro:Primary/Secondary ds:UpToDate/Inconsistent C r-----
    ns:16437312 nr:0 dw:22602340 dr:59475304 al:256 bm:3653 lo:1 pe:123 ua:64 ap:0 ep:1 wo:f oos:4555336
        &lt;span class="o"&gt;[==============&lt;/span&gt;&amp;gt;.....&lt;span class="o"&gt;]&lt;/span&gt; synced: 78.3% &lt;span class="o"&gt;(&lt;/span&gt;4448/20480&lt;span class="o"&gt;)&lt;/span&gt;Mfinish: 0:01:14 speed: 61,144 &lt;span class="o"&gt;(&lt;/span&gt;58,212&lt;span class="o"&gt;)&lt;/span&gt; K/sec
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Verify the disks now&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;gnt-instance info &lt;span class="nv"&gt;$instance&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep drbd
  Disk template: drbd
    - disk/0: drbd8, size 20.0G
      on primary:   /dev/drbd1 &lt;span class="o"&gt;(&lt;/span&gt;147:1&lt;span class="o"&gt;)&lt;/span&gt; in sync, status ok
      on secondary: /dev/drbd1 &lt;span class="o"&gt;(&lt;/span&gt;147:1&lt;span class="o"&gt;)&lt;/span&gt; in sync, status ok
&lt;/pre&gt;&lt;/div&gt;</summary><category term="ganeti"></category><category term="kvm"></category><category term="drbd"></category></entry><entry><title>Evacuating a Ganeti node for hardware diagnostics</title><link href="http://blkperl.github.com/evac-ganeti-node.html" rel="alternate"></link><updated>2013-03-23T12:00:00-07:00</updated><author><name>William Van Hevelingen</name></author><id>tag:blkperl.github.com,2013-03-23:evac-ganeti-node.html</id><summary type="html">&lt;p&gt;One of the nodes in our Ganeti cluster was hanging on lvs commands and some of the instances were hanging while they wait for IO. I decided to be proactive and live migrate all of the instances off the node in order to bring it down for some debugging.&lt;/p&gt;
&lt;p&gt;The following steps are how I brought the node offline with minimal downtime for production instances.&lt;/p&gt;
&lt;h2&gt;Overview of our cluster&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;4 nodes&lt;/li&gt;
&lt;li&gt;46 instances&lt;/li&gt;
&lt;li&gt;Ubuntu 12.04 LTS&lt;/li&gt;
&lt;li&gt;Ganeti version 2.5.2-1&lt;/li&gt;
&lt;li&gt;Kvm version 1.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Log into the master node&lt;/h2&gt;
&lt;p&gt;Log into the master ganeti node.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;ssh nebula.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Migrate the primary instances off suspect node&lt;/h2&gt;
&lt;p&gt;Run the gnt-node migrate command, passing in the node to migrate off of&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-node migrate katana.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output should look similar to this.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;root@claymore:~# gnt-node migrate katana
Migrate instance&lt;span class="o"&gt;(&lt;/span&gt;s&lt;span class="o"&gt;)&lt;/span&gt; crystal.cat.pdx.edu, l1011.cat.pdx.edu,
marauder.cat.pdx.edu, nyan.cat.pdx.edu, panic.cat.pdx.edu,
receptacle.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu,
webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu?
y/&lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;/?: y
Submitted &lt;span class="nb"&gt;jobs &lt;/span&gt;52820, 52821, 52822, 52823, 52824, 52825, 52826, 52827, 52828, 52829, 52830
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
Waiting &lt;span class="k"&gt;for&lt;/span&gt; job &lt;span class="m"&gt;52825&lt;/span&gt; ...
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:33:52 &lt;span class="m"&gt;2013&lt;/span&gt; Migrating instance crystal.cat.pdx.edu
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:33:52 &lt;span class="m"&gt;2013&lt;/span&gt; * checking disk consistency between &lt;span class="nb"&gt;source &lt;/span&gt;and target
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:33:56 &lt;span class="m"&gt;2013&lt;/span&gt; * switching node claymore.cat.pdx.edu to secondary mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:33:56 &lt;span class="m"&gt;2013&lt;/span&gt; * changing into standalone mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:34:58 &lt;span class="m"&gt;2013&lt;/span&gt; * changing disks into dual-master mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:01 &lt;span class="m"&gt;2013&lt;/span&gt; * &lt;span class="nb"&gt;wait &lt;/span&gt;&lt;span class="k"&gt;until&lt;/span&gt; resync is &lt;span class="k"&gt;done&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:02 &lt;span class="m"&gt;2013&lt;/span&gt; * preparing claymore.cat.pdx.edu to accept the instance
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:03 &lt;span class="m"&gt;2013&lt;/span&gt; * migrating instance to claymore.cat.pdx.edu
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:24 &lt;span class="m"&gt;2013&lt;/span&gt; * switching node katana.cat.pdx.edu to secondary mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:26 &lt;span class="m"&gt;2013&lt;/span&gt; * &lt;span class="nb"&gt;wait &lt;/span&gt;&lt;span class="k"&gt;until&lt;/span&gt; resync is &lt;span class="k"&gt;done&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:27 &lt;span class="m"&gt;2013&lt;/span&gt; * changing into standalone mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:27 &lt;span class="m"&gt;2013&lt;/span&gt; * changing disks into single-master mode
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:28 &lt;span class="m"&gt;2013&lt;/span&gt; * &lt;span class="nb"&gt;wait &lt;/span&gt;&lt;span class="k"&gt;until&lt;/span&gt; resync is &lt;span class="k"&gt;done&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:35:28 &lt;span class="m"&gt;2013&lt;/span&gt; * &lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Some of my instances failed to migrate properly. This seems to be related to this &lt;a href="https://code.google.com/p/ganeti/issues/detail?id=297"&gt;bug&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 10:33:16 &lt;span class="m"&gt;2013&lt;/span&gt; Migration failed, aborting
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
Job &lt;span class="m"&gt;52820&lt;/span&gt; has failed: Failure: &lt;span class="nb"&gt;command &lt;/span&gt;execution error:
Could not migrate instance sapphire.cat.pdx.edu: Failed to migrate instance: Too many &lt;span class="s1"&gt;&amp;#39;info migrate&amp;#39;&lt;/span&gt; broken answers
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Luckly these instances were not being used at the moment and I could use gnt-instance failover.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Warning! This will reboot the instance&lt;/span&gt;
sudo gnt-instance failover sapphire.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Evacuate secondaries using gnt-node&lt;/h2&gt;
&lt;p&gt;After the primaries have been migrated run the gnt-node evacuate command to move the secondaries.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-node evacuate --secondary-only node2.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;gnt-node evacuate failed with a timeout error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-node evacuate --secondary-only katana
Relocate instance&lt;span class="o"&gt;(&lt;/span&gt;s&lt;span class="o"&gt;)&lt;/span&gt; crystal.cat.pdx.edu, emerald.cat.pdx.edu,
&lt;span class="o"&gt;[&lt;/span&gt;...&lt;span class="o"&gt;]&lt;/span&gt;
y/&lt;span class="o"&gt;[&lt;/span&gt;n&lt;span class="o"&gt;]&lt;/span&gt;/?: y
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 11:16:49 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Evacuating instances from node &lt;span class="s1"&gt;&amp;#39;katana.cat.pdx.edu&amp;#39;&lt;/span&gt;: crystal.cat.pdx.edu, emerald.cat.pdx.edu, fog.cat.pdx.edu, gameandwatch.cat.pdx.edu, l1011.cat.pdx.edu, log.cat.pdx.edu, marauder.cat.pdx.edu, nydus.cat.pdx.edu, panic.cat.pdx.edu, pika.cat.pdx.edu, receptacle.cat.pdx.edu, refinery.cat.pdx.edu, ruby.cat.pdx.edu, sapphire.cat.pdx.edu, void.cat.pdx.edu, warpgate.cat.pdx.edu, weba.cat.pdx.edu, webb.cat.pdx.edu, webc.cat.pdx.edu, webd.cat.pdx.edu, yermom.cat.pdx.edu, zeratul.cat.pdx.edu
Failure: &lt;span class="nb"&gt;command &lt;/span&gt;execution error:
Can&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;t get data &lt;span class="k"&gt;for&lt;/span&gt; node katana.cat.pdx.edu: Error 28: Operation timed out after &lt;span class="m"&gt;60910&lt;/span&gt; milliseconds with &lt;span class="m"&gt;0&lt;/span&gt; out of -1 bytes received
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If gnt-node evacuate fails for you can evacuate secondaries per instance.&lt;/p&gt;
&lt;h2&gt;Evacuate the secondaries using gnt-instance&lt;/h2&gt;
&lt;p&gt;Locate the instances you want to move.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-instance list -o name,pnode,snodes &lt;span class="p"&gt;|&lt;/span&gt; grep katana

gameandwatch.cat.pdx.edu claymore.cat.pdx.edu katana.cat.pdx.edu
l1011.cat.pdx.edu        rapier.cat.pdx.edu   katana.cat.pdx.edu
log.cat.pdx.edu          dirk.cat.pdx.edu     katana.cat.pdx.edu
marauder.cat.pdx.edu     rapier.cat.pdx.edu   katana.cat.pdx.edu
yermom.cat.pdx.edu       dirk.cat.pdx.edu     katana.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run gnt-instance replace-disks and give the name of another node that is not the primary of the instance.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-instance replace-disks -n claymore.cat.pdx.edu yermom.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output will look similar to this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:39 &lt;span class="m"&gt;2013&lt;/span&gt; Replacing disk&lt;span class="o"&gt;(&lt;/span&gt;s&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; yermom.cat.pdx.edu
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:39 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 1/6 Check device existence
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:39 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Checking disk/0 on rapier.cat.pdx.edu
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:40 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Checking volume groups
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:40 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 2/6 Check peer consistency
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:40 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Checking disk/0 consistency on node rapier.cat.pdx.edu
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:40 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 3/6 Allocate new storage
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:40 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Adding new &lt;span class="nb"&gt;local &lt;/span&gt;storage on claymore.cat.pdx.edu &lt;span class="k"&gt;for&lt;/span&gt; disk/0
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:44 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 4/6 Changing drbd configuration
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:44 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: activating a new drbd on claymore.cat.pdx.edu &lt;span class="k"&gt;for&lt;/span&gt; disk/0
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:48 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Shutting down drbd &lt;span class="k"&gt;for&lt;/span&gt; disk/0 on old node
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:48 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Detaching primary drbds from the network &lt;span class="o"&gt;(=&lt;/span&gt;&amp;gt; standalone&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:49 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Updating instance configuration
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:49 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Attaching primary drbds to new secondary &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;standalone&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;&amp;gt; connected&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:50 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 5/6 Sync devices
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:50 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Waiting &lt;span class="k"&gt;for&lt;/span&gt; instance yermom.cat.pdx.edu to sync disks.
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:24:55 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0:  0.50% &lt;span class="k"&gt;done&lt;/span&gt;, 13m 34s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:26:00 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0:  6.90% &lt;span class="k"&gt;done&lt;/span&gt;, 15m 53s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:27:07 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 13.40% &lt;span class="k"&gt;done&lt;/span&gt;, 14m 42s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:28:26 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 21.00% &lt;span class="k"&gt;done&lt;/span&gt;, 13m 1s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:29:30 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 27.00% &lt;span class="k"&gt;done&lt;/span&gt;, 14m 32s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:30:35 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 33.20% &lt;span class="k"&gt;done&lt;/span&gt;, 11m 27s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:31:40 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 39.50% &lt;span class="k"&gt;done&lt;/span&gt;, 9m 56s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:32:44 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 45.80% &lt;span class="k"&gt;done&lt;/span&gt;, 9m 20s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:32:44 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 45.80% &lt;span class="k"&gt;done&lt;/span&gt;, 9m 20s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:33:49 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 52.10% &lt;span class="k"&gt;done&lt;/span&gt;, 8m 2s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:34:54 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 58.30% &lt;span class="k"&gt;done&lt;/span&gt;, 6m 54s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:35:58 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 64.60% &lt;span class="k"&gt;done&lt;/span&gt;, 5m 48s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:37:03 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 70.90% &lt;span class="k"&gt;done&lt;/span&gt;, 4m 52s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:38:08 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 77.20% &lt;span class="k"&gt;done&lt;/span&gt;, 3m 50s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:39:15 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 83.70% &lt;span class="k"&gt;done&lt;/span&gt;, 2m 33s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:40:20 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 90.00% &lt;span class="k"&gt;done&lt;/span&gt;, 1m 44s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:41:25 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: - device disk/0: 96.30% &lt;span class="k"&gt;done&lt;/span&gt;, 37s remaining &lt;span class="o"&gt;(&lt;/span&gt;estimated&lt;span class="o"&gt;)&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:42:04 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Instance yermom.cat.pdx.edu&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;s disks are in sync.
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:42:04 &lt;span class="m"&gt;2013&lt;/span&gt; STEP 6/6 Removing old storage
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:42:04 &lt;span class="m"&gt;2013&lt;/span&gt;  - INFO: Remove logical volumes &lt;span class="k"&gt;for&lt;/span&gt; 0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Sometimes you may get an lvm error that looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:15:18 &lt;span class="m"&gt;2013&lt;/span&gt;  - WARNING: Can&lt;span class="s1"&gt;&amp;#39;t remove old LV: Can&amp;#39;&lt;/span&gt;t lvremove: exited with &lt;span class="nb"&gt;exit &lt;/span&gt;code &lt;span class="m"&gt;5&lt;/span&gt; -   Unable to deactivate open ganeti-9bc9b089--59f3-
-4512--9a09--e7f756caadbe.disk0_meta &lt;span class="o"&gt;(&lt;/span&gt;252:7&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;  Unable to deactivate logical volume &lt;span class="s2"&gt;&amp;quot;9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;
Wed Mar &lt;span class="m"&gt;20&lt;/span&gt; 13:15:18 &lt;span class="m"&gt;2013&lt;/span&gt;       Hint: remove unused LVs manually
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If this happens you can manually remove the old volume with the lvremove command&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Log into the node&lt;/span&gt;
ssh katana.cat.pdx.edu
lvremove /dev/mapper/ganeti-9bc9b089-59f3-4512-9a09-e7f756caadbe.disk0_meta
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Mark the node offline&lt;/h2&gt;
&lt;p&gt;The last step is to mark the node offline&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;sudo gnt-node modify -O yes katana.cat.pdx.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;gnt-cluster verify should now display that one node is offline&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;gnt-cluster verify 
&lt;span class="o"&gt;[&lt;/span&gt;..&lt;span class="o"&gt;]&lt;/span&gt;
Sat Mar &lt;span class="m"&gt;23&lt;/span&gt; 14:18:14 &lt;span class="m"&gt;2013&lt;/span&gt; * Other Notes
Sat Mar &lt;span class="m"&gt;23&lt;/span&gt; 14:18:14 &lt;span class="m"&gt;2013&lt;/span&gt;   - NOTICE: &lt;span class="m"&gt;1&lt;/span&gt; offline node&lt;span class="o"&gt;(&lt;/span&gt;s&lt;span class="o"&gt;)&lt;/span&gt; found.
&lt;span class="o"&gt;[&lt;/span&gt;..&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And that is it, you can now take the node offline for diagnostics or to wait for new parts.&lt;/p&gt;
&lt;h2&gt;Rebalance the cluster&lt;/h2&gt;
&lt;p&gt;At this point you probably want to rebalance your Ganeti cluser. The following blog post by Lance Albertson does a pretty good job at explaining this process.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/"&gt;http://www.lancealbertson.com/2011/05/rebalancing-ganeti-clusters/&lt;/a&gt;&lt;/p&gt;</summary><category term="ganeti"></category><category term="kvm"></category></entry></feed>